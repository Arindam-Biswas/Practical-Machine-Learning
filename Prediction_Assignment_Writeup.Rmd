---
title: "Prediction Assignment Write-up"
author: "Arindam Biswas"
date: "October 25, 2015"
output: html_document
---

##Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. People regularly quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants, who were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har.

The goal of your project is to predict the manner in which they did the exercise. We implement the Random Forests model. Before apply the prediction model on the data-set, we clean the data by performing various data cleansing steps. 

We evaluate that the accuracy of the model on training and validation data-set is 99.69%. This is a very high accuracy level, hence we predict the `classe` variable for the test data set using this prediction model.

##Prepare the Environment and Load Libraries

First we prepare the environment and load the required libraries for the predictive modelling and set seed

```{r environ, warning=FALSE, message=FALSE}
library(caret)
library(randomForest)
library(parallel)
library(doParallel)
set.seed(1234)
```

##Load Data

The training data for this project is available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and the test data [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

```{r load}
#set the URLs of the training and testing data
trainurl ="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

#set the names of the training and test data
trainname = "pml-training.csv"
testname = "pml-testing.csv"

#if the data files do not exist, download the files
if (!file.exists(trainname)) {
  download.file(trainurl, destfile=trainname, method="curl")
}

if (!file.exists(testname)) {
  download.file(testurl, destfile=testname, method="curl")
}

# load the CSV files as data.frame 
train = read.csv("pml-training.csv", header=TRUE, sep=",", na.strings=c("", "NA", "NULL"))
test = read.csv("pml-testing.csv", header=TRUE, sep=",", na.strings=c("", "NA", "NULL"))
```

##Data Exploration and Cleansing

```{r explore, results='hide'}
str(train)
head(train,5)
```

```{r dimen}
dim(train)
dim(test)
```

The raw training data has 19622 rows of observations and 160 columns. While the testing data has 20 rows and 160 columns. There is one column/variable to predict named `classe`. 

After studying the training data, we perform the below cleansing of the data:

- Remove variables that have too many NA values (less than 80% of data filled)
- Remove variables not relevant for classification (`X`, `user_name`, `raw_timestamp_part_1`, `raw_timestamp_part_2`, `cvtd_timestamp`, `new_window`, `num_window`)
- Remove variables that have extremely low variance
- Convert `classe` into factor

```{r clean}
#remove variables with less than 80% of data filled
filled <- c((colSums(!is.na(train[,-ncol(train)])) >= 0.8*nrow(train)))
train <- train[,filled]
test <- test[,filled]
dim(train)

#remove variables not relevant for classification
nonrelevant = c('X', 'user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp', 'new_window', 'num_window')
train <- train[, -which(names(train) %in% nonrelevant)]
test <- test[, -which(names(test) %in% nonrelevant)]

#remove variables that have extremely low variance
lowVar <- nearZeroVar(train[sapply(train, is.numeric)], saveMetrics = TRUE)
train <- train[, lowVar$nzv==FALSE]
test <- test[, lowVar$nzv==FALSE]

#convert classe into factor
train$classe <- factor(train$classe)
dim(train)
```

The cleansed training data has 19622 rows of observations and 53 columns. 

##Splitting Data

Split the train data into two partitions of training and validating data sets in the ratio 70:30.

```{r split}
split <- createDataPartition(y=train$classe, p=0.7, list=FALSE)
trainPart <- train[split,]
validPart <- train[-split,]
dim(trainPart)
dim(validPart)
```

We get 13737 samples and 53 variables for training, 5885 samples and 53 variables for validating

##Modelling and Evaluation

Now we use the Random Forest model on the trainPart data split.

```{r model}
rfmodel <- randomForest(classe~., data=trainPart, importance = TRUE)
rfmodel
```

Our Random Forest model shows OOB estimate of error rate: 0.47% for the training data. 

Now we verify the variable importance measures as produced by random Forest.

```{r eval}
varImpPlot(rfmodel)
```

From the plots we see which variables have higher impact on the prediction.

##Out-of Sample Error

The Random Forest model shows OOB estimate of error rate: 0.47% for the training data split. Now we will predict it for out-of sample accuracy by evaluating our model results through confusion Matrix on the validation data split.

```{r conf}
confusionMatrix(predict(rfmodel,newdata=validPart[,-ncol(validPart)]),validPart$classe)
```

We observe that the Random Forest model accuracy as tested over Validation data split is 99.69%, which is a very good accuracy level.

##Cross Validation

To improve the model and more specifically to avoid over-fitting, we employ cross validation with 10 folds.

```{r crossval}
registerDoParallel(makeCluster(detectCores()))  
controlCV <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
rfmodel_CV <- randomForest(classe~., data=trainPart, importance = TRUE, trControl = controlCV)
```

Now we calculate the revised accuracy of the model with cross validation.

```{r confCV}
confusionMatrix(predict(rfmodel_CV,newdata=validPart[,-ncol(validPart)]),validPart$classe)
```

With cross validation the accuracy is 99.68%, which is almost identical to the baseline accuracy (99.69%) which was already very high.

##Predict on Test Data

Finally, we predict the new values using the model on the testing csv provided. 

```{r predict}
predictions <- predict(rfmodel,newdata=test)
```